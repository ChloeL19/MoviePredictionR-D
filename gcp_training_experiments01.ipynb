{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chloeloughridge/anaconda/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# going to try training the highest performing normalized LSTM model with GCP\n",
    "\n",
    "\n",
    "# First, import necessary libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# setting up the keras stuff\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import LSTM\n",
    "# my custom data_utils file\n",
    "from data_utils_local08 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# interfacing with GCP\n",
    "\n",
    "# collecting some potentially helpful commands . . . \n",
    "# for saving matrices directly to gcp:\n",
    "#np.save(file_io.FileIO('gs://my-bucket/123', 'w'), np.array([[1,2,3], [4,5,6]]))\n",
    "    # may also want to try with just plain old np.save command . . . not sure what file_io does\n",
    "#I also want to learn how to actually train a model using Cloud TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 212, 248832)\n"
     ]
    }
   ],
   "source": [
    "# uploading the X values\n",
    "#X_input = load_Xinput(get_fc6_directory(7)) #VGG16 data\n",
    "\n",
    "X_input = np.load('../Desktop/InceptionResNetV2_feature_data.npy') #resnet data!\n",
    "print(X_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 219)\n"
     ]
    }
   ],
   "source": [
    "# uploading the Y values\n",
    "y_data_input = np.load('../Desktop/labels.npy')\n",
    "print(y_data_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 212, 248832) (4, 212, 248832) (4, 212) (3, 212)\n"
     ]
    }
   ],
   "source": [
    "timesteps = X_input.shape[1]\n",
    "data_dim = X_input.shape[2]\n",
    "\n",
    "X_train = X_input[:4,:timesteps, :]\n",
    "Y_train = y_data_input[:4, :timesteps]\n",
    "\n",
    "X_test = X_input[4:, :timesteps, :]\n",
    "Y_test = y_data_input[4:, :timesteps]\n",
    "\n",
    "print(X_test.shape, X_train.shape, Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# THE OLD MODEL IN KERAS\n",
    "\n",
    "# constructing a many-to-one LSTM model in keras --> inspiration: https://stackoverflow.com/questions/43034960/many-to-one-and-many-to-many-lstm-examples-in-keras\n",
    "# i will start by training a model on only the VGG16 fc6 layer output (that's just one feature)\n",
    "# should I eventually abstract this LSTM model? Create its own object file?\n",
    "model = Sequential()\n",
    "\n",
    "# normalization\n",
    "model.add(BatchNormalization(input_shape=(timesteps, data_dim)))\n",
    "\n",
    "model.add(LSTM(212, return_sequences=True))\n",
    "#input_shape=(timesteps, data_dim)\n",
    "\n",
    "# going to try adding a flatten layer in here\n",
    "model.add(Flatten()) # I got this from a github thing, but I still don't completely understand why it works\n",
    "# add the final dense layer and then softmax\n",
    "model.add(Dense(212, activation='sigmoid'))\n",
    "# going to add a softmax activation to this\n",
    "#model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COMPILATION OF THE OLD KERAS MODEL\n",
    "# compiling LSTM model\n",
    "# note that Ng used an Adam optimizer and categorical cross-entropy loss\n",
    "# but this is a binary classification problem so I think the parameters below should suffice\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['binary_accuracy', FScore2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a new metric of evaluation! the F-score!\n",
    "def FScore2(y_true, y_pred):\n",
    "    '''\n",
    "    The F score, beta=2\n",
    "    '''\n",
    "    B2 = K.variable(4)\n",
    "    OnePlusB2 = K.variable(5)\n",
    "    pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(K.less(K.abs(pred - K.clip(y_true, .5, 1.)), 0.01), 'float32'), -1)\n",
    "    fp = K.sum(K.cast(K.greater(pred - y_true, 0.1), 'float32'), -1)\n",
    "    fn = K.sum(K.cast(K.less(pred - y_true, -0.1), 'float32'), -1)\n",
    "\n",
    "    f2 = OnePlusB2 * tp / (OnePlusB2 * tp + B2 * fn + fp)\n",
    "\n",
    "    return K.mean(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4 samples, validate on 3 samples\n",
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "# running the LSTM model\n",
    "model.fit(X_train, Y_train, epochs = 20, batch_size = 128, validation_data=(X_test, Y_test))\n",
    "print(\"finished training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# START OF NEW MODEL THAT IS COMPATIBLE WITH GCP\n",
    "\n",
    "# setting up Cloud TPU stuff \n",
    "# following code from here: https://github.com/tensorflow/tpu/blob/master/models/experimental/cifar_keras/cifar_keras.py\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from absl import flags\n",
    "import absl.logging as _logging  # pylint: disable=unused-import\n",
    "import tensorflow as tf\n",
    "\n",
    "# Cloud TPU Cluster Resolvers\n",
    "flags.DEFINE_string(\n",
    "    'tpu', default=None,\n",
    "    help='The Cloud TPU to use for training. This should be either the name '\n",
    "    'used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 url.')\n",
    "flags.DEFINE_string(\n",
    "    \"gcp_project\", default=None,\n",
    "    help=\"Project name for the Cloud TPU-enabled project. If not specified, we \"\n",
    "    \"will attempt to automatically detect the GCE project from metadata.\")\n",
    "flags.DEFINE_string(\n",
    "    \"tpu_zone\", default=None,\n",
    "    help=\"GCE zone where the Cloud TPU is located in. If not specified, we \"\n",
    "    \"will attempt to automatically detect the GCE project from metadata.\")\n",
    "\n",
    "# Model specific paramenters\n",
    "flags.DEFINE_integer(\"batch_size\", 128,\n",
    "                     \"Mini-batch size for the computation. Note that this \"\n",
    "                     \"is the global batch size and not the per-shard batch.\")\n",
    "flags.DEFINE_float(\"learning_rate\", 0.05, \"Learning rate.\")\n",
    "flags.DEFINE_string(\"train_file\", \"\", \"Path to cifar10 training data.\")\n",
    "flags.DEFINE_integer(\"train_steps\", 100000,\n",
    "                     \"Total number of steps. Note that the actual number of \"\n",
    "                     \"steps is the next multiple of --iterations greater \"\n",
    "                     \"than this value.\")\n",
    "flags.DEFINE_bool(\"use_tpu\", True, \"Use TPUs rather than plain CPUs\")\n",
    "flags.DEFINE_string(\"model_dir\", None, \"Estimator model_dir\")\n",
    "flags.DEFINE_integer(\"iterations_per_loop\", 100,\n",
    "                     \"Number of iterations per TPU training loop.\")\n",
    "flags.DEFINE_integer(\"num_shards\", 8, \"Number of shards (TPU chips).\")\n",
    "\n",
    "\n",
    "FLAGS = flags.FLAGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-349045626524>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# trying to view the model output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model prediction:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"target:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_data' is not defined"
     ]
    }
   ],
   "source": [
    "# trying to view the model output\n",
    "out = model.predict(X_train_data)\n",
    "print(\"model prediction:\")\n",
    "print(out[0])\n",
    "print(\"target:\")\n",
    "print(Y_train_data[0])\n",
    "\n",
    "print(\"before 64:\")\n",
    "print(out[0][63])\n",
    "print(\"64:\")\n",
    "print(out[0][65])\n",
    "\n",
    "print(\"rounded\")\n",
    "print(np.round(out)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#try visualizing this model at some point?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
