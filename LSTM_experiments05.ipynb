{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This version uses abstracted functions that actually work to load X and Y training data into arrays of the right shape\n",
    "# data successfully flows through LSTM connected to a flatten and dense layer\n",
    "# this version is trained on all of part 1 data and the model is saved in a local file\n",
    "\n",
    "# Here I will be building out the architecture of the first classification LSTM\n",
    "# At each time step, this LSTM will take in a vector representing the extracted audio and visual features from Liris-ACCEDE\n",
    "# Its goal is to output whether or not the movie induces fear at each time step\n",
    "\n",
    "\n",
    "# First, import necessary libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# setting up the keras stuff\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers import LSTM\n",
    "# my custom data_utils file\n",
    "from data_utils_local08 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(212, 4096)\n"
     ]
    }
   ],
   "source": [
    "# uploading the X values\n",
    "X_input = load_Xinput(get_fc6_directory(7))\n",
    "print(X_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(212,)\n"
     ]
    }
   ],
   "source": [
    "# uploading the Y values\n",
    "y_data_input = fear_oneHot(212, 'fear_annotations_part01/MEDIAEVAL18_7_Fear.txt')\n",
    "print(y_data_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 212, 4096)\n"
     ]
    }
   ],
   "source": [
    "# Uploading part01 of the local fc6 data\n",
    "\n",
    "timesteps = 212   # the number of seconds in movie 07 --> i will figure out how to programmatically get this value\n",
    "data_dim = 4096    # the number of output values from VGG16 layer fc6 --> switch to programmatic later\n",
    "num_movies = 4\n",
    "batch_size = 7\n",
    "num_epochs = 5\n",
    "validation_num = 3\n",
    "\n",
    "# set up the X_train_data master array\n",
    "X_train_data = np.zeros([num_movies, timesteps, data_dim]) #oooooh this array will have to be as long as the longest\n",
    "                                                            # movie and padded with zeros --> this won't cause problems\n",
    "                                                            # right?\n",
    "X_valid_data = np.zeros([validation_num, timesteps, data_dim])\n",
    "        \n",
    "# for each movie number between and including 7 and 13\n",
    "for num in range(num_movies):\n",
    "    # load the X_input data\n",
    "    X_input = load_Xinput(get_fc6_directory(7 + num))\n",
    "    # put this X_input data into the X_train_data array\n",
    "    X_train_data[num, :, :] = X_input\n",
    "print(X_train_data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 212, 4096)\n"
     ]
    }
   ],
   "source": [
    "# loading X validation set\n",
    "X_valid_data = np.zeros([validation_num, timesteps, data_dim])\n",
    "\n",
    "for num in range(validation_num):\n",
    "    # load the X_input data\n",
    "    X_valid = load_Xinput(get_fc6_directory(7 + num_movies + num))\n",
    "    # put this X_input data into the X_train_data array\n",
    "    X_valid_data[num, :, :] = X_valid\n",
    "print(X_valid_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/chloeloughridge/AIGrant_LSTMdev\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# uploading the y data\n",
    "\n",
    "# set up y_train_data master array\n",
    "Y_train_data = np.zeros([num_movies, timesteps])\n",
    "\n",
    "# for each movie number between and including 7 and 13\n",
    "for num in range(num_movies):\n",
    "    # create the appropriate path to the fear annotation data\n",
    "    #print(num)\n",
    "    path = os.path.join('fear_annotations_part01/MEDIAEVAL18_{}_Fear.txt'.format(7+num))\n",
    "    # create a one-hot vector\n",
    "    y_data = fear_oneHot(timesteps, path)\n",
    "    # add this one-hot vector to y_train_data\n",
    "    Y_train_data[num, :] = y_data\n",
    "print(Y_train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# upload the Y validation set\n",
    "Y_valid_data = np.zeros([validation_num, timesteps])\n",
    "\n",
    "# for each movie number in validation set\n",
    "for num in range(validation_num):\n",
    "    # create the appropriate path to the fear annotation data\n",
    "    #print(num)\n",
    "    path = os.path.join('fear_annotations_part01/MEDIAEVAL18_{}_Fear.txt'.format(7+ num_movies + num))\n",
    "    # create a one-hot vector\n",
    "    y_valid = fear_oneHot(timesteps, path)\n",
    "    # add this one-hot vector to y_train_data\n",
    "    Y_valid_data[num, :] = y_valid\n",
    "print(Y_valid_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# constructing a many-to-one LSTM model in keras --> inspiration: https://stackoverflow.com/questions/43034960/many-to-one-and-many-to-many-lstm-examples-in-keras\n",
    "# i will start by training a model on only the VGG16 fc6 layer output (that's just one feature)\n",
    "# should I eventually abstract this LSTM model? Create its own object file?\n",
    "model = Sequential()\n",
    "model.add(LSTM(212, return_sequences=True,\n",
    "               input_shape=(timesteps, data_dim)))\n",
    "# going to try adding a flatten layer in here\n",
    "model.add(Flatten()) # I got this from a github thing, but I still don't completely understand why it works\n",
    "# add the final dense layer and then softmax\n",
    "model.add(Dense(212, activation='sigmoid'))\n",
    "# going to add a softmax activation to this\n",
    "#model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a new metric of evaluation! the F-score!\n",
    "def FScore2(y_true, y_pred):\n",
    "    '''\n",
    "    The F score, beta=2\n",
    "    '''\n",
    "    B2 = K.variable(4)\n",
    "    OnePlusB2 = K.variable(5)\n",
    "    pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(K.less(K.abs(pred - K.clip(y_true, .5, 1.)), 0.01), 'float32'), -1)\n",
    "    fp = K.sum(K.cast(K.greater(pred - y_true, 0.1), 'float32'), -1)\n",
    "    fn = K.sum(K.cast(K.less(pred - y_true, -0.1), 'float32'), -1)\n",
    "\n",
    "    f2 = OnePlusB2 * tp / (OnePlusB2 * tp + B2 * fn + fp)\n",
    "\n",
    "    return K.mean(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compiling LSTM model\n",
    "# note that Ng used an Adam optimizer and categorical cross-entropy loss\n",
    "# but this is a binary classification problem so I think the parameters below should suffice\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['binary_accuracy', FScore2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4 samples, validate on 3 samples\n",
      "Epoch 1/20\n",
      "4/4 [==============================] - 3s 862ms/step - loss: 4.0847e-06 - binary_accuracy: 1.0000 - FScore2: 1.0000 - val_loss: 2.3184 - val_binary_accuracy: 0.7091 - val_FScore2: 0.1421\n",
      "Epoch 2/20\n",
      "4/4 [==============================] - 2s 601ms/step - loss: 1.5648e-06 - binary_accuracy: 1.0000 - FScore2: 1.0000 - val_loss: 2.3866 - val_binary_accuracy: 0.7013 - val_FScore2: 0.1224\n",
      "Epoch 3/20\n",
      "4/4 [==============================] - 2s 612ms/step - loss: 4.2172e-07 - binary_accuracy: 1.0000 - FScore2: 1.0000 - val_loss: 2.4186 - val_binary_accuracy: 0.7060 - val_FScore2: 0.1201\n",
      "Epoch 4/20\n",
      "4/4 [==============================] - 2s 601ms/step - loss: 3.0463e-07 - binary_accuracy: 1.0000 - FScore2: 1.0000 - val_loss: 2.4412 - val_binary_accuracy: 0.7044 - val_FScore2: 0.1235\n",
      "Epoch 5/20\n",
      "4/4 [==============================] - 2s 609ms/step - loss: 2.4863e-07 - binary_accuracy: 1.0000 - FScore2: 1.0000 - val_loss: 2.4482 - val_binary_accuracy: 0.7060 - val_FScore2: 0.1235\n",
      "Epoch 6/20\n",
      "4/4 [==============================] - 3s 758ms/step - loss: 2.2667e-07 - binary_accuracy: 1.0000 - FScore2: 1.0000 - val_loss: 2.4579 - val_binary_accuracy: 0.7075 - val_FScore2: 0.1204\n",
      "Epoch 7/20\n",
      "4/4 [==============================] - 3s 638ms/step - loss: 2.1148e-07 - binary_accuracy: 1.0000 - FScore2: 1.0000 - val_loss: 2.4628 - val_binary_accuracy: 0.7060 - val_FScore2: 0.1136\n",
      "Epoch 8/20\n",
      "4/4 [==============================] - 3s 626ms/step - loss: 1.9984e-07 - binary_accuracy: 1.0000 - FScore2: 1.0000 - val_loss: 2.4687 - val_binary_accuracy: 0.7091 - val_FScore2: 0.1136\n",
      "Epoch 9/20\n",
      "4/4 [==============================] - 3s 635ms/step - loss: 1.8915e-07 - binary_accuracy: 1.0000 - FScore2: 1.0000 - val_loss: 2.4730 - val_binary_accuracy: 0.7091 - val_FScore2: 0.1136\n",
      "Epoch 10/20\n",
      "4/4 [==============================] - 3s 689ms/step - loss: 1.8070e-07 - binary_accuracy: 1.0000 - FScore2: 1.0000 - val_loss: 2.4772 - val_binary_accuracy: 0.7123 - val_FScore2: 0.1141\n",
      "Epoch 11/20\n",
      "4/4 [==============================] - 3s 630ms/step - loss: 1.7432e-07 - binary_accuracy: 1.0000 - FScore2: 1.0000 - val_loss: 2.4813 - val_binary_accuracy: 0.7123 - val_FScore2: 0.1141\n",
      "Epoch 12/20\n",
      "4/4 [==============================] - 3s 679ms/step - loss: 1.6877e-07 - binary_accuracy: 1.0000 - FScore2: 1.0000 - val_loss: 2.4844 - val_binary_accuracy: 0.7138 - val_FScore2: 0.1175\n",
      "Epoch 13/20\n",
      "4/4 [==============================] - 3s 670ms/step - loss: 1.6278e-07 - binary_accuracy: 1.0000 - FScore2: 1.0000 - val_loss: 2.4877 - val_binary_accuracy: 0.7138 - val_FScore2: 0.1175\n",
      "Epoch 14/20\n",
      "4/4 [==============================] - 3s 710ms/step - loss: 1.5840e-07 - binary_accuracy: 1.0000 - FScore2: 1.0000 - val_loss: 2.4907 - val_binary_accuracy: 0.7123 - val_FScore2: 0.1141\n",
      "Epoch 15/20\n",
      "4/4 [==============================] - 3s 626ms/step - loss: 1.5430e-07 - binary_accuracy: 1.0000 - FScore2: 1.0000 - val_loss: 2.4935 - val_binary_accuracy: 0.7138 - val_FScore2: 0.1141\n",
      "Epoch 16/20\n",
      "4/4 [==============================] - 2s 624ms/step - loss: 1.5077e-07 - binary_accuracy: 1.0000 - FScore2: 1.0000 - val_loss: 2.4947 - val_binary_accuracy: 0.7138 - val_FScore2: 0.1141\n",
      "Epoch 17/20\n",
      "4/4 [==============================] - 3s 653ms/step - loss: 1.4751e-07 - binary_accuracy: 1.0000 - FScore2: 1.0000 - val_loss: 2.4965 - val_binary_accuracy: 0.7138 - val_FScore2: 0.1141\n",
      "Epoch 18/20\n",
      "4/4 [==============================] - 3s 706ms/step - loss: 1.4487e-07 - binary_accuracy: 1.0000 - FScore2: 1.0000 - val_loss: 2.4970 - val_binary_accuracy: 0.7138 - val_FScore2: 0.1141\n",
      "Epoch 19/20\n",
      "4/4 [==============================] - 3s 701ms/step - loss: 1.4265e-07 - binary_accuracy: 1.0000 - FScore2: 1.0000 - val_loss: 2.4982 - val_binary_accuracy: 0.7123 - val_FScore2: 0.1106\n",
      "Epoch 20/20\n",
      "4/4 [==============================] - 3s 632ms/step - loss: 1.3994e-07 - binary_accuracy: 1.0000 - FScore2: 1.0000 - val_loss: 2.4992 - val_binary_accuracy: 0.7123 - val_FScore2: 0.1106\n",
      "finished training!\n"
     ]
    }
   ],
   "source": [
    "# running the LSTM model\n",
    "model.fit(X_train_data, Y_train_data, epochs = 20, validation_data=(X_valid_data, Y_valid_data))\n",
    "print(\"finished training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model prediction:\n",
      "[2.25139465e-05 9.77822765e-07 1.55764565e-06 9.70406109e-06\n",
      " 6.14908140e-06 2.91141873e-06 5.22274013e-06 1.31821944e-05\n",
      " 3.59225123e-06 1.60548734e-05 1.79300896e-05 1.42105273e-05\n",
      " 6.05305013e-06 1.40088814e-05 2.13852745e-05 3.10756695e-06\n",
      " 3.44362502e-06 1.65802576e-05 1.54017562e-05 3.70630232e-06\n",
      " 5.05536718e-06 1.67455983e-05 6.35322112e-06 1.66260652e-05\n",
      " 1.50669193e-05 3.79963308e-06 2.20809379e-05 1.12312728e-05\n",
      " 1.93098022e-05 1.42896597e-05 3.82243934e-06 2.33241171e-05\n",
      " 4.73718392e-04 2.56292173e-04 1.77335933e-05 2.45983887e-04\n",
      " 8.03970124e-06 1.66992960e-03 1.09345077e-04 3.60266713e-04\n",
      " 2.22114377e-05 5.07977093e-04 4.92297841e-06 4.86800127e-04\n",
      " 5.38802260e-06 8.06433491e-06 1.23085229e-05 6.87312968e-06\n",
      " 1.83838012e-04 2.77133251e-04 7.47776676e-06 1.53146088e-04\n",
      " 9.04848450e-04 2.22479124e-04 1.79624883e-04 4.24964412e-04\n",
      " 1.01324520e-03 1.35171483e-03 1.84225471e-04 1.22018108e-04\n",
      " 2.12393847e-04 4.49114188e-04 2.76052597e-04 6.12464100e-06\n",
      " 9.99006569e-01 9.97664452e-01 9.98207092e-01 9.98870790e-01\n",
      " 9.97622192e-01 9.98482287e-01 9.99744713e-01 9.97887313e-01\n",
      " 9.99876499e-01 9.99739349e-01 9.99521017e-01 9.98772919e-01\n",
      " 9.99506593e-01 9.99047101e-01 9.99290347e-01 9.99696732e-01\n",
      " 9.98407781e-01 9.99554574e-01 9.99830842e-01 9.99104798e-01\n",
      " 9.99921560e-01 9.98804927e-01 9.98107195e-01 9.98782694e-01\n",
      " 9.99576151e-01 9.99092817e-01 9.98091757e-01 9.99232054e-01\n",
      " 9.98401225e-01 9.99134123e-01 9.98765945e-01 9.98445213e-01\n",
      " 9.98039305e-01 9.97979820e-01 9.98378873e-01 9.99277890e-01\n",
      " 9.98584509e-01 9.98857021e-01 1.07670867e-05 4.08611435e-04\n",
      " 4.07583457e-06 9.99168515e-01 9.97132659e-01 9.98733461e-01\n",
      " 9.98445094e-01 9.98457313e-01 6.11734959e-06 4.93687345e-04\n",
      " 1.79690149e-04 1.84193341e-04 1.29999215e-04 2.60170083e-04\n",
      " 2.91263103e-04 2.37056869e-04 2.31217477e-04 1.86226157e-06\n",
      " 2.58080458e-04 4.62776916e-06 1.73733220e-04 1.92109110e-05\n",
      " 2.48612138e-04 5.33949060e-04 3.13255828e-06 1.03551438e-05\n",
      " 1.44560590e-05 9.98707056e-01 9.98195827e-01 9.99204218e-01\n",
      " 9.99223471e-01 9.99413133e-01 9.99158025e-01 9.98185337e-01\n",
      " 9.98675644e-01 9.99121726e-01 9.99048412e-01 9.98418689e-01\n",
      " 9.98291194e-01 9.98750925e-01 9.98486400e-01 9.99099612e-01\n",
      " 9.98040736e-01 9.98026073e-01 9.18780916e-06 1.63198783e-05\n",
      " 3.07320602e-06 1.17208920e-05 1.42917452e-05 7.82816005e-06\n",
      " 7.88159923e-06 4.28317326e-05 2.48560946e-06 2.20041761e-06\n",
      " 1.10817900e-05 5.10283144e-06 5.79172638e-06 4.44617672e-06\n",
      " 4.20305423e-06 2.00691411e-05 6.78014385e-06 7.67168603e-06\n",
      " 9.98575160e-06 4.16827788e-06 2.02059327e-05 6.69172732e-06\n",
      " 2.90086746e-06 3.26392455e-06 9.71234385e-07 1.11385589e-05\n",
      " 6.15860017e-06 1.19029810e-05 1.49953030e-05 1.34302945e-05\n",
      " 7.22714049e-06 4.28322210e-06 9.28337158e-06 5.39156463e-06\n",
      " 1.81053019e-05 6.00493740e-06 2.81242737e-05 1.04632963e-05\n",
      " 1.58795938e-05 2.98264172e-06 1.70485673e-05 6.44496640e-06\n",
      " 3.95044617e-06 1.12577818e-05 8.77506136e-06 1.30013241e-05\n",
      " 1.71257743e-05 3.69961868e-04 3.13212455e-04 3.58702062e-04\n",
      " 9.01679050e-06 1.33601370e-05 2.42389069e-05 2.43937302e-06\n",
      " 7.69084181e-06 7.07520758e-06 7.07783283e-06 1.96491710e-05\n",
      " 5.18446905e-06 2.44957300e-05 1.09264465e-05 2.25453527e-06\n",
      " 6.22830203e-06 1.03290859e-05 6.96250891e-06 1.59151605e-05]\n",
      "target:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "before 64:\n",
      "6.124641e-06\n",
      "64:\n",
      "0.99766445\n",
      "rounded\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# trying to view the model output\n",
    "out = model.predict(X_train_data)\n",
    "print(\"model prediction:\")\n",
    "print(out[0])\n",
    "print(\"target:\")\n",
    "print(Y_train_data[0])\n",
    "\n",
    "print(\"before 64:\")\n",
    "print(out[0][63])\n",
    "print(\"64:\")\n",
    "print(out[0][65])\n",
    "\n",
    "print(\"rounded\")\n",
    "print(np.round(out)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#try visualizing this model at some point?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
