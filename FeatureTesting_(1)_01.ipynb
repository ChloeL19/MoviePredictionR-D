{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LSTM+CNN+Dropout\n",
    "# trained on a variety of feature combinations\n",
    "\n",
    "# First, import necessary libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# setting up the keras stuff\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, Conv1D, BatchNormalization\n",
    "from keras.layers import LSTM\n",
    "# my custom data_utils file\n",
    "from data_utils_local08 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -4.479     0.6116   -2.4313  ...   0.40712  -1.26     -0.38584]\n",
      " [ -4.4253    0.43527  -2.4657  ...   0.40969  -1.3558   -0.44199]\n",
      " [ -5.7516   -7.8904   -6.8481  ... -10.082    -9.5151    5.0102 ]\n",
      " ...\n",
      " [-10.531     1.5749   -2.5768  ...   3.0096   -4.0217    5.9333 ]\n",
      " [ -9.3       3.5598   -2.054   ...   4.5326   -5.1147    5.7942 ]\n",
      " [ -4.6847    2.7427   -3.7783  ...   1.6169    1.38      3.8095 ]]\n"
     ]
    }
   ],
   "source": [
    "# uploading the X values from the autoencoder\n",
    "X_input = np.load('../Desktop/VGG16_feature_data.npy')\n",
    "print(X_input[1])\n",
    "\n",
    "# normalize X_input\n",
    "\n",
    "# for combinations of features\n",
    "#X_input = np.concatenate((X_input_VAE, X_input_VGG), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(212,)\n"
     ]
    }
   ],
   "source": [
    "# uploading the Y values\n",
    "y_data_input = fear_oneHot(212, 'fear_annotations_part01/MEDIAEVAL18_7_Fear.txt')\n",
    "print(y_data_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 212, 4096)\n",
      "(3, 212, 4096)\n"
     ]
    }
   ],
   "source": [
    "# partition data into training and testing sets\n",
    "train_num = 4\n",
    "val_num = 3\n",
    "\n",
    "X_train_data = X_input[:train_num, :, :]\n",
    "X_valid_data = X_input[train_num:train_num+val_num, :, :]\n",
    "\n",
    "print(X_train_data.shape)\n",
    "print(X_valid_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# uploading the y training data\n",
    "timesteps = X_train_data.shape[1]\n",
    "data_dim = X_train_data.shape[2]\n",
    "\n",
    "# set up y_train_data master array\n",
    "Y_train_data = np.zeros([train_num, timesteps])\n",
    "\n",
    "# for each movie number between and including 7 and 13\n",
    "for num in range(train_num):\n",
    "    # create the appropriate path to the fear annotation data\n",
    "    #print(num)\n",
    "    path = os.path.join('fear_annotations_part01/MEDIAEVAL18_{}_Fear.txt'.format(7+num))\n",
    "    # create a one-hot vector\n",
    "    y_data = fear_oneHot(timesteps, path)\n",
    "    # add this one-hot vector to y_train_data\n",
    "    Y_train_data[num, :] = y_data\n",
    "print(Y_train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# upload the Y validation set\n",
    "\n",
    "Y_valid_data = np.zeros([val_num, timesteps])\n",
    "\n",
    "# for each movie number in validation set\n",
    "for num in range(val_num):\n",
    "    # create the appropriate path to the fear annotation data\n",
    "    #print(num)\n",
    "    path = os.path.join('fear_annotations_part01/MEDIAEVAL18_{}_Fear.txt'.format(7+ train_num + num))\n",
    "    # create a one-hot vector\n",
    "    y_valid = fear_oneHot(timesteps, path)\n",
    "    # add this one-hot vector to y_train_data\n",
    "    Y_valid_data[num, :] = y_valid\n",
    "print(Y_valid_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model architecture\n",
    "model = Sequential()\n",
    "\n",
    "# normalization layer\n",
    "#model.add(BatchNormalization(input_shape=(timesteps, data_dim)))\n",
    "\n",
    "model.add(Conv1D(25, kernel_size=3, activation=\"sigmoid\", input_shape=(timesteps, data_dim)))\n",
    "# input_shape=(timesteps, data_dim)\n",
    "\n",
    "model.add(LSTM(timesteps, return_sequences=True))\n",
    "# input_shape=(timesteps, data_dim)\n",
    "# dropout layer\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# another LSTM layer\n",
    "model.add(LSTM(timesteps, return_sequences=True))\n",
    "\n",
    "# necessary flatten layer\n",
    "model.add(Flatten()) \n",
    "\n",
    "# add the final dense layer and then softmax\n",
    "model.add(Dense(timesteps, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a new metric of evaluation! the F-score!\n",
    "def FScore2(y_true, y_pred):\n",
    "    '''\n",
    "    The F score, beta=2\n",
    "    '''\n",
    "    B2 = K.variable(4)\n",
    "    OnePlusB2 = K.variable(5)\n",
    "    pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(K.less(K.abs(pred - K.clip(y_true, .5, 1.)), 0.01), 'float32'), -1)\n",
    "    fp = K.sum(K.cast(K.greater(pred - y_true, 0.1), 'float32'), -1)\n",
    "    fn = K.sum(K.cast(K.less(pred - y_true, -0.1), 'float32'), -1)\n",
    "\n",
    "    f2 = OnePlusB2 * tp / (OnePlusB2 * tp + B2 * fn + fp)\n",
    "\n",
    "    return K.mean(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# another attempt at f1 score\n",
    "def precision(y_true, y_pred):\n",
    "    #\"\"\"Precision metric.\n",
    "    # Only computes a batch-wise average of precision.\n",
    "    #Computes the precision, a metric for multi-label classification of\n",
    "    #how many selected items are relevant.\n",
    "    #\"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "def recall(y_true, y_pred):\n",
    "    #\"\"\"Recall metric.\n",
    "    # Only computes a batch-wise average of recall.\n",
    "    # Computes the recall, a metric for multi-label classification of\n",
    "    #how many relevant items are selected.\n",
    "    #\"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def fbeta_score(y_true, y_pred, beta=1):\n",
    "    #\"\"\"Computes the F score.\n",
    "     #The F score is the weighted harmonic mean of precision and recall.\n",
    "    #Here it is only computed as a batch-wise average, not globally.\n",
    "    # This is useful for multi-label classification, where input samples can be\n",
    "    #classified as sets of labels. By only using accuracy (precision) a model\n",
    "    #would achieve a perfect score by simply assigning every class to every\n",
    "    #input. In order to avoid this, a metric should penalize incorrect class\n",
    "    #assignments as well (recall). The F-beta score (ranged from 0.0 to 1.0)\n",
    "    #computes this, as a weighted mean of the proportion of correct class\n",
    "    #assignments vs. the proportion of incorrect class assignments.\n",
    "    # With beta = 1, this is equivalent to a F-measure. With beta < 1, assigning\n",
    "    #correct classes becomes more important, and with beta > 1 the metric is\n",
    "    #instead weighted towards penalizing incorrect class assignments.\n",
    "    #\"\"\"\n",
    "    if beta < 0:\n",
    "        raise ValueError('The lowest choosable beta is zero (only precision).')\n",
    "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
    "    return fbeta_score\n",
    "\n",
    "def fmeasure(y_true, y_pred):\n",
    "    #\"\"\"Computes the f-measure, the harmonic mean of precision and recall.\n",
    "    #Here it is only computed as a batch-wise average, not globally.\n",
    "    #\"\"\"\n",
    "    return fbeta_score(y_true, y_pred, beta=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compiling LSTM model\n",
    "# note that Ng used an Adam optimizer and categorical cross-entropy loss\n",
    "# but this is a binary classification problem so I think the parameters below should suffice\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['binary_accuracy', FScore2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4 samples, validate on 3 samples\n",
      "Epoch 1/20\n",
      "4/4 [==============================] - 2s 625ms/step - loss: 0.8115 - binary_accuracy: 0.8054 - FScore2: 0.0000e+00 - val_loss: 3.1164 - val_binary_accuracy: 0.7689 - val_FScore2: 0.0000e+00\n",
      "Epoch 2/20\n",
      "4/4 [==============================] - 1s 218ms/step - loss: 1.6969 - binary_accuracy: 0.8066 - FScore2: 0.1389 - val_loss: 1.4172 - val_binary_accuracy: 0.7689 - val_FScore2: 0.0000e+00\n",
      "Epoch 3/20\n",
      "4/4 [==============================] - 1s 353ms/step - loss: 1.1336 - binary_accuracy: 0.8054 - FScore2: 0.0000e+00 - val_loss: 1.7805 - val_binary_accuracy: 0.7689 - val_FScore2: 0.0000e+00\n",
      "Epoch 4/20\n",
      "4/4 [==============================] - 1s 344ms/step - loss: 1.1147 - binary_accuracy: 0.8066 - FScore2: 0.1389 - val_loss: 1.4467 - val_binary_accuracy: 0.7689 - val_FScore2: 0.0000e+00\n",
      "Epoch 5/20\n",
      "4/4 [==============================] - 1s 300ms/step - loss: 0.9160 - binary_accuracy: 0.8054 - FScore2: 0.0000e+00 - val_loss: 1.5370 - val_binary_accuracy: 0.7689 - val_FScore2: 0.0000e+00\n",
      "Epoch 6/20\n",
      "4/4 [==============================] - 1s 325ms/step - loss: 0.8727 - binary_accuracy: 0.8054 - FScore2: 0.0000e+00 - val_loss: 1.4612 - val_binary_accuracy: 0.7689 - val_FScore2: 0.0000e+00\n",
      "Epoch 7/20\n",
      "4/4 [==============================] - 1s 280ms/step - loss: 0.8236 - binary_accuracy: 0.8054 - FScore2: 0.0000e+00 - val_loss: 1.5486 - val_binary_accuracy: 0.7689 - val_FScore2: 0.0000e+00\n",
      "Epoch 8/20\n",
      "4/4 [==============================] - 1s 235ms/step - loss: 0.8071 - binary_accuracy: 0.8054 - FScore2: 0.0000e+00 - val_loss: 1.5778 - val_binary_accuracy: 0.7689 - val_FScore2: 0.0000e+00\n",
      "Epoch 9/20\n",
      "4/4 [==============================] - 1s 293ms/step - loss: 0.8031 - binary_accuracy: 0.8054 - FScore2: 0.0000e+00 - val_loss: 1.6224 - val_binary_accuracy: 0.7689 - val_FScore2: 0.0000e+00\n",
      "Epoch 10/20\n",
      "4/4 [==============================] - 1s 227ms/step - loss: 0.8044 - binary_accuracy: 0.8066 - FScore2: 0.1389 - val_loss: 1.6269 - val_binary_accuracy: 0.7689 - val_FScore2: 0.0000e+00\n",
      "Epoch 11/20\n",
      "4/4 [==============================] - 1s 295ms/step - loss: 0.8089 - binary_accuracy: 0.8066 - FScore2: 0.1389 - val_loss: 1.6765 - val_binary_accuracy: 0.7689 - val_FScore2: 0.0000e+00\n",
      "Epoch 12/20\n",
      "4/4 [==============================] - 1s 241ms/step - loss: 0.8193 - binary_accuracy: 0.8066 - FScore2: 0.1389 - val_loss: 1.6100 - val_binary_accuracy: 0.7689 - val_FScore2: 0.0000e+00\n",
      "Epoch 13/20\n",
      "4/4 [==============================] - 1s 220ms/step - loss: 0.8132 - binary_accuracy: 0.8066 - FScore2: 0.1389 - val_loss: 1.6542 - val_binary_accuracy: 0.7689 - val_FScore2: 0.0000e+00\n",
      "Epoch 14/20\n",
      "4/4 [==============================] - 1s 257ms/step - loss: 0.8075 - binary_accuracy: 0.8066 - FScore2: 0.1389 - val_loss: 1.6285 - val_binary_accuracy: 0.7689 - val_FScore2: 0.0000e+00\n",
      "Epoch 15/20\n",
      "4/4 [==============================] - 1s 221ms/step - loss: 0.8056 - binary_accuracy: 0.8054 - FScore2: 0.0000e+00 - val_loss: 1.6547 - val_binary_accuracy: 0.7689 - val_FScore2: 0.0000e+00\n",
      "Epoch 16/20\n",
      "4/4 [==============================] - 1s 309ms/step - loss: 0.8049 - binary_accuracy: 0.8054 - FScore2: 0.0000e+00 - val_loss: 1.6394 - val_binary_accuracy: 0.7689 - val_FScore2: 0.0000e+00\n",
      "Epoch 17/20\n",
      "4/4 [==============================] - 1s 296ms/step - loss: 0.8072 - binary_accuracy: 0.8054 - FScore2: 0.0000e+00 - val_loss: 1.6459 - val_binary_accuracy: 0.7689 - val_FScore2: 0.0000e+00\n",
      "Epoch 18/20\n",
      "4/4 [==============================] - 1s 237ms/step - loss: 0.8058 - binary_accuracy: 0.8066 - FScore2: 0.1389 - val_loss: 1.6237 - val_binary_accuracy: 0.7689 - val_FScore2: 0.0000e+00\n",
      "Epoch 19/20\n",
      "4/4 [==============================] - 1s 258ms/step - loss: 0.8028 - binary_accuracy: 0.8066 - FScore2: 0.1389 - val_loss: 1.6536 - val_binary_accuracy: 0.7689 - val_FScore2: 0.0000e+00\n",
      "Epoch 20/20\n",
      "4/4 [==============================] - 1s 319ms/step - loss: 0.8028 - binary_accuracy: 0.8066 - FScore2: 0.1389 - val_loss: 1.6186 - val_binary_accuracy: 0.7689 - val_FScore2: 0.0000e+00\n",
      "finished training!\n"
     ]
    }
   ],
   "source": [
    "# running the LSTM model\n",
    "model.fit(X_train_data, Y_train_data, epochs = 20, validation_data=(X_valid_data, Y_valid_data))\n",
    "print(\"finished training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model prediction:\n",
      "[2.38958673e-05 2.63057191e-05 2.37448912e-05 2.68017175e-05\n",
      " 2.60396464e-05 2.54288461e-05 2.33226492e-05 2.71342287e-05\n",
      " 2.43517934e-05 3.10983633e-05 2.43608974e-05 2.32270286e-05\n",
      " 2.27290620e-05 2.73130590e-05 2.68658223e-05 2.47956687e-05\n",
      " 2.33350183e-05 2.44454714e-05 2.45334722e-05 2.64859809e-05\n",
      " 2.14069860e-05 2.43648028e-05 2.34919680e-05 2.40291447e-05\n",
      " 2.40296704e-05 2.44170224e-05 9.06482019e-05 8.71397860e-05\n",
      " 8.60483851e-05 1.33781679e-04 9.07979702e-05 8.94258337e-05\n",
      " 9.07842914e-05 8.57495397e-05 8.38751512e-05 1.19831377e-04\n",
      " 8.37490152e-05 5.97990453e-01 3.45843554e-01 9.91230991e-05\n",
      " 9.12500618e-05 9.07953727e-05 8.89375660e-05 1.42566263e-04\n",
      " 1.27050414e-04 9.37161312e-05 8.16072352e-05 9.36284050e-05\n",
      " 9.39250531e-05 8.68483330e-05 8.91739182e-05 1.41794735e-04\n",
      " 9.89116845e-04 1.30350981e-03 1.39874732e-03 1.21719681e-03\n",
      " 1.30413775e-03 1.33993581e-03 1.16660981e-03 1.16246229e-03\n",
      " 1.64742698e-04 8.43369198e-05 7.85895900e-05 9.31399263e-05\n",
      " 6.35892153e-04 5.54589264e-04 6.93408481e-04 5.55889448e-04\n",
      " 5.98549552e-04 1.92986813e-03 2.10448168e-03 1.77289080e-03\n",
      " 2.08422379e-03 2.09163898e-03 1.42997640e-04 1.36731978e-04\n",
      " 2.35843865e-04 2.25060954e-04 1.70790823e-04 1.87358906e-04\n",
      " 1.47907250e-03 1.50591263e-03 1.89183571e-03 1.39214389e-03\n",
      " 1.89857720e-03 1.52633176e-04 1.64306199e-04 1.65108708e-04\n",
      " 1.76388494e-04 1.48659004e-04 1.77479553e-04 1.60366326e-04\n",
      " 1.31465262e-04 1.58421230e-04 1.72769447e-04 5.98445360e-04\n",
      " 2.00273050e-03 1.91983255e-03 2.19280622e-03 5.66439470e-04\n",
      " 8.37666390e-04 6.68038265e-04 8.55047256e-05 1.02573678e-04\n",
      " 9.01822350e-05 5.32607548e-04 5.60385641e-04 5.92527911e-04\n",
      " 5.88577939e-04 5.92932105e-04 8.39540517e-05 1.05432904e-04\n",
      " 8.72037126e-05 1.16658863e-04 1.21534256e-04 8.90225056e-05\n",
      " 1.01929989e-04 1.47081417e-04 7.95832675e-05 9.24488631e-05\n",
      " 1.55822112e-04 8.55746330e-05 1.28624684e-04 9.77188683e-05\n",
      " 9.60765246e-05 8.71148513e-05 2.44934490e-05 2.15557975e-05\n",
      " 2.72962625e-05 2.15675187e-04 1.84129021e-04 1.61411677e-04\n",
      " 1.50277847e-04 1.79571463e-04 2.24158095e-04 1.64543599e-04\n",
      " 1.67752616e-04 1.86663689e-04 1.53029046e-04 1.78912480e-04\n",
      " 1.27837877e-04 1.44815494e-04 2.15501146e-04 1.73662454e-04\n",
      " 1.68275932e-04 1.83487442e-04 2.70311502e-05 2.27102737e-05\n",
      " 2.26152242e-05 2.52244863e-05 2.10496528e-05 2.24327669e-05\n",
      " 2.38164666e-05 2.50382363e-05 2.73056612e-05 2.51031033e-05\n",
      " 2.67124306e-05 2.74632966e-05 2.35826101e-05 2.42103306e-05\n",
      " 2.78966054e-05 2.56282856e-05 2.53444723e-05 2.92875757e-05\n",
      " 2.39334749e-05 2.52297759e-05 2.42694423e-05 2.45906740e-05\n",
      " 2.96956132e-05 2.28286208e-05 2.46451891e-05 2.38169214e-05\n",
      " 2.75838029e-05 2.26993179e-05 2.32252769e-05 2.57228021e-05\n",
      " 2.75896964e-05 2.59512381e-05 2.38818120e-05 2.33249211e-05\n",
      " 2.68414406e-05 2.22913968e-05 2.58656746e-05 2.39384317e-05\n",
      " 2.39840847e-05 2.51828533e-05 2.24117248e-05 2.43716113e-05\n",
      " 2.46371292e-05 2.68809417e-05 2.72368452e-05 2.63820930e-05\n",
      " 9.94995789e-05 1.76962218e-04 8.01496426e-05 8.21139765e-05\n",
      " 1.69607869e-04 2.34550989e-05 2.42189908e-05 2.62584199e-05\n",
      " 2.40516347e-05 2.25065251e-05 2.27490327e-05 2.67500345e-05\n",
      " 2.12846826e-05 2.49798504e-05 2.34689505e-05 2.26182001e-05\n",
      " 2.40935769e-05 2.63852144e-05 2.41477719e-05 2.63093571e-05\n",
      " 2.33872652e-05 2.54914412e-05 2.45635092e-05 2.47453518e-05\n",
      " 2.29087254e-05 2.88439915e-05 2.49461154e-05]\n",
      "target:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "before 64:\n",
      "0.00010816625\n",
      "64:\n",
      "0.01952434\n",
      "rounded\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# trying to view the model output\n",
    "out = model.predict(X_train_data)\n",
    "print(\"model prediction:\")\n",
    "print(out[2])\n",
    "print(\"target:\")\n",
    "print(Y_train_data[0])\n",
    "\n",
    "print(\"before 64:\")\n",
    "print(out[0][63])\n",
    "print(\"64:\")\n",
    "print(out[0][65])\n",
    "\n",
    "print(\"rounded\")\n",
    "print(np.round(out)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#try visualizing this model at some point?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
