{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LSTM+CNN+Dropout\n",
    "# trained on VAE and VGG16 feature data\n",
    "\n",
    "# First, import necessary libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# setting up the keras stuff\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, Conv1D\n",
    "from keras.layers import LSTM\n",
    "# my custom data_utils file\n",
    "from data_utils_local08 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 212, 12288)\n",
      "(7, 212, 4096)\n",
      "(7, 212, 16384)\n"
     ]
    }
   ],
   "source": [
    "# uploading the X values from the autoencoder\n",
    "X_input_auto = np.load('./auto_output01.npy')\n",
    "X_input_VAE = X_input_auto[:,:212,:]\n",
    "print(X_input_VAE.shape)\n",
    "X_input_VGG = np.load('../Desktop/VGG16_feature_data.npy')\n",
    "print(X_input_VGG.shape)\n",
    "X_input = np.concatenate((X_input_VAE, X_input_VGG), axis=2)\n",
    "print(X_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(212,)\n"
     ]
    }
   ],
   "source": [
    "# uploading the Y values\n",
    "y_data_input = fear_oneHot(212, 'fear_annotations_part01/MEDIAEVAL18_7_Fear.txt')\n",
    "print(y_data_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 212, 16384)\n",
      "(2, 212, 16384)\n"
     ]
    }
   ],
   "source": [
    "# partition data into training and testing sets\n",
    "train_num = 5\n",
    "val_num = 2\n",
    "\n",
    "X_train_data = X_input[:train_num, :, :]\n",
    "X_valid_data = X_input[train_num:train_num+val_num, :, :]\n",
    "\n",
    "print(X_train_data.shape)\n",
    "print(X_valid_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# uploading the y training data\n",
    "timesteps = X_train_data.shape[1]\n",
    "data_dim = X_train_data.shape[2]\n",
    "\n",
    "# set up y_train_data master array\n",
    "Y_train_data = np.zeros([train_num, timesteps])\n",
    "\n",
    "# for each movie number between and including 7 and 13\n",
    "for num in range(train_num):\n",
    "    # create the appropriate path to the fear annotation data\n",
    "    #print(num)\n",
    "    path = os.path.join('fear_annotations_part01/MEDIAEVAL18_{}_Fear.txt'.format(7+num))\n",
    "    # create a one-hot vector\n",
    "    y_data = fear_oneHot(timesteps, path)\n",
    "    # add this one-hot vector to y_train_data\n",
    "    Y_train_data[num, :] = y_data\n",
    "print(Y_train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n",
      " 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# upload the Y validation set\n",
    "\n",
    "Y_valid_data = np.zeros([val_num, timesteps])\n",
    "\n",
    "# for each movie number in validation set\n",
    "for num in range(val_num):\n",
    "    # create the appropriate path to the fear annotation data\n",
    "    #print(num)\n",
    "    path = os.path.join('fear_annotations_part01/MEDIAEVAL18_{}_Fear.txt'.format(7+ train_num + num))\n",
    "    # create a one-hot vector\n",
    "    y_valid = fear_oneHot(timesteps, path)\n",
    "    # add this one-hot vector to y_train_data\n",
    "    Y_valid_data[num, :] = y_valid\n",
    "print(Y_valid_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model architecture\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(10, kernel_size=3, activation=\"sigmoid\", input_shape=(timesteps, data_dim)))\n",
    "\n",
    "model.add(LSTM(timesteps, return_sequences=True))\n",
    "# input_shape=(timesteps, data_dim)\n",
    "# dropout layer\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# another LSTM layer\n",
    "model.add(LSTM(timesteps, return_sequences=True))\n",
    "\n",
    "# necessary flatten layer\n",
    "model.add(Flatten()) \n",
    "\n",
    "# add the final dense layer and then softmax\n",
    "model.add(Dense(timesteps, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compiling LSTM model\n",
    "# note that Ng used an Adam optimizer and categorical cross-entropy loss\n",
    "# but this is a binary classification problem so I think the parameters below should suffice\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5 samples, validate on 2 samples\n",
      "Epoch 1/20\n",
      "5/5 [==============================] - 4s 715ms/step - loss: 0.9386 - binary_accuracy: 0.8255 - val_loss: 2.5915 - val_binary_accuracy: 0.7005\n",
      "Epoch 2/20\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.9319 - binary_accuracy: 0.8255 - val_loss: 3.1845 - val_binary_accuracy: 0.7005\n",
      "Epoch 3/20\n",
      "5/5 [==============================] - 1s 261ms/step - loss: 2.5433 - binary_accuracy: 0.8255 - val_loss: 2.8892 - val_binary_accuracy: 0.7005\n",
      "Epoch 4/20\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 1.6602 - binary_accuracy: 0.8255 - val_loss: 2.2245 - val_binary_accuracy: 0.7005\n",
      "Epoch 5/20\n",
      "5/5 [==============================] - 1s 284ms/step - loss: 1.0707 - binary_accuracy: 0.8255 - val_loss: 1.8602 - val_binary_accuracy: 0.7005\n",
      "Epoch 6/20\n",
      "5/5 [==============================] - 2s 326ms/step - loss: 1.0275 - binary_accuracy: 0.8255 - val_loss: 1.7146 - val_binary_accuracy: 0.7005\n",
      "Epoch 7/20\n",
      "5/5 [==============================] - 2s 309ms/step - loss: 0.8581 - binary_accuracy: 0.8255 - val_loss: 1.7961 - val_binary_accuracy: 0.7005\n",
      "Epoch 8/20\n",
      "5/5 [==============================] - 1s 272ms/step - loss: 0.8339 - binary_accuracy: 0.8255 - val_loss: 1.8546 - val_binary_accuracy: 0.7005\n",
      "Epoch 9/20\n",
      "5/5 [==============================] - 1s 298ms/step - loss: 0.8346 - binary_accuracy: 0.8255 - val_loss: 1.8739 - val_binary_accuracy: 0.7005\n",
      "Epoch 10/20\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.8283 - binary_accuracy: 0.8255 - val_loss: 1.9397 - val_binary_accuracy: 0.7005\n",
      "Epoch 11/20\n",
      "5/5 [==============================] - 1s 260ms/step - loss: 0.8234 - binary_accuracy: 0.8255 - val_loss: 1.9111 - val_binary_accuracy: 0.7005\n",
      "Epoch 12/20\n",
      "5/5 [==============================] - 2s 321ms/step - loss: 0.8214 - binary_accuracy: 0.8255 - val_loss: 2.0712 - val_binary_accuracy: 0.7005\n",
      "Epoch 13/20\n",
      "5/5 [==============================] - 2s 397ms/step - loss: 0.8245 - binary_accuracy: 0.8255 - val_loss: 1.8717 - val_binary_accuracy: 0.7005\n",
      "Epoch 14/20\n",
      "5/5 [==============================] - 2s 394ms/step - loss: 0.8599 - binary_accuracy: 0.8255 - val_loss: 1.9224 - val_binary_accuracy: 0.7005\n",
      "Epoch 15/20\n",
      "5/5 [==============================] - 2s 409ms/step - loss: 0.8128 - binary_accuracy: 0.8255 - val_loss: 1.8755 - val_binary_accuracy: 0.7005\n",
      "Epoch 16/20\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.7999 - binary_accuracy: 0.8255 - val_loss: 1.8738 - val_binary_accuracy: 0.7005\n",
      "Epoch 17/20\n",
      "5/5 [==============================] - 1s 281ms/step - loss: 0.7891 - binary_accuracy: 0.8255 - val_loss: 1.9763 - val_binary_accuracy: 0.7005\n",
      "Epoch 18/20\n",
      "5/5 [==============================] - 2s 336ms/step - loss: 0.7815 - binary_accuracy: 0.8255 - val_loss: 1.8768 - val_binary_accuracy: 0.7005\n",
      "Epoch 19/20\n",
      "5/5 [==============================] - 1s 280ms/step - loss: 0.7768 - binary_accuracy: 0.8255 - val_loss: 2.0090 - val_binary_accuracy: 0.7005\n",
      "Epoch 20/20\n",
      "5/5 [==============================] - 1s 282ms/step - loss: 0.7560 - binary_accuracy: 0.8255 - val_loss: 1.8898 - val_binary_accuracy: 0.7005\n",
      "finished training!\n"
     ]
    }
   ],
   "source": [
    "# running the LSTM model\n",
    "model.fit(X_train_data, Y_train_data, epochs = 20, validation_data=(X_valid_data, Y_valid_data))\n",
    "print(\"finished training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# trying to view the model output\n",
    "out = model.predict(X_train_data)\n",
    "print(\"model prediction:\")\n",
    "print(out[0])\n",
    "print(\"target:\")\n",
    "print(Y_train_data[0])\n",
    "\n",
    "print(\"before 64:\")\n",
    "print(out[0][63])\n",
    "print(\"64:\")\n",
    "print(out[0][65])\n",
    "\n",
    "print(\"rounded\")\n",
    "print(np.round(out)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#try visualizing this model at some point?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
